{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A simple library of functions that provide feature importances\n",
    "for scikit-learn random forest regressors and classifiers.\n",
    "MIT License\n",
    "Terence Parr, http://parrt.cs.usfca.edu\n",
    "Kerem Turgutlu, https://www.linkedin.com/in/kerem-turgutlu-12906b65\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "if LooseVersion(sklearn.__version__) >= LooseVersion(\"0.24\"):\n",
    "    # In sklearn version 0.24, forest module changed to be private.\n",
    "    from sklearn.ensemble._forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import _forest as forest\n",
    "else:\n",
    "    # Before sklearn version 0.24, forest was public, supporting this.\n",
    "    from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "    from sklearn.ensemble import forest\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from copy import copy\n",
    "import warnings\n",
    "import tempfile\n",
    "from os import getpid, makedirs\n",
    "\n",
    "GREY = '#444443'\n",
    "\n",
    "__version__='1.3.7'\n",
    "\n",
    "\n",
    "class PimpViz:\n",
    "    \"\"\"\n",
    "    For use with jupyter notebooks, plot_importances returns an instance\n",
    "    of this class so we display SVG not PNG.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        tmp = tempfile.gettempdir()\n",
    "        self.svgfilename = tmp+\"/PimpViz_\"+str(getpid())+\".svg\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.svgfilename, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    def _repr_svg_(self):\n",
    "        with open(self.svgfilename, \"r\", encoding='UTF-8') as f:\n",
    "            svg = f.read()\n",
    "        plt.close()\n",
    "        return svg\n",
    "\n",
    "    def save(self, filename):\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    def view(self):\n",
    "        plt.show()\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def importances(model, X_valid, y_valid, features=None, n_samples=5000, sort=True, metric=None, sample_weights = None):\n",
    "    \"\"\"\n",
    "    Compute permutation feature importances for scikit-learn models using\n",
    "    a validation set.\n",
    "    Given a Classifier or Regressor in model\n",
    "    and validation X and y data, return a data frame with columns\n",
    "    Feature and Importance sorted in reverse order by importance.\n",
    "    The validation data is needed to compute model performance\n",
    "    measures (accuracy or R^2). The model is not retrained.\n",
    "    You can pass in a list with a subset of features interesting to you.\n",
    "    All unmentioned features will be grouped together into a single meta-feature\n",
    "    on the graph. You can also pass in a list that has sublists like:\n",
    "    [['latitude', 'longitude'], 'price', 'bedrooms']. Each string or sublist\n",
    "    will be permuted together as a feature or meta-feature; the drop in\n",
    "    overall accuracy of the model is the relative importance.\n",
    "    The model.score() method is called to measure accuracy drops.\n",
    "    This version that computes accuracy drops with the validation set\n",
    "    is much faster than the OOB, cross validation, or drop column\n",
    "    versions. The OOB version is a less vectorized because it needs to dig\n",
    "    into the trees to get out of examples. The cross validation and drop column\n",
    "    versions need to do retraining and are necessarily much slower.\n",
    "    This function used OOB not validation sets in 1.0.5; switched to faster\n",
    "    test set version for 1.0.6. (breaking API change)\n",
    "    :param model: The scikit model fit to training data\n",
    "    :param X_valid: Data frame with feature vectors of the validation set\n",
    "    :param y_valid: Series with target variable of validation set\n",
    "    :param features: The list of features to show in importance graph.\n",
    "                     These can be strings (column names) or lists of column\n",
    "                     names. E.g., features = ['bathrooms', ['latitude', 'longitude']].\n",
    "                     Feature groups can overlap, with features appearing in multiple.\n",
    "    :param n_samples: How many records of the validation set to use\n",
    "                      to compute permutation importance. The default is\n",
    "                      5000, which we arrived at by experiment over a few data sets.\n",
    "                      As we cannot be sure how all data sets will react,\n",
    "                      you can pass in whatever sample size you want. Pass in -1\n",
    "                      to mean entire validation set. Our experiments show that\n",
    "                      not too many records are needed to get an accurate picture of\n",
    "                      feature importance.\n",
    "    :param sort: Whether to sort the resulting importances\n",
    "    :param metric: Metric in the form of callable(model, X_valid, y_valid, sample_weights) to evaluate for,\n",
    "                    if not set default's to model.score()\n",
    "    :param sample_weights: set if a different weighting is required for the validation samples\n",
    "    return: A data frame with Feature, Importance columns\n",
    "    SAMPLE CODE\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    X_train, y_train = ..., ...\n",
    "    X_valid, y_valid = ..., ...\n",
    "    rf.fit(X_train, y_train)\n",
    "    imp = importances(rf, X_valid, y_valid)\n",
    "    \"\"\"\n",
    "    def flatten(features):\n",
    "        all_features = set()\n",
    "        for sublist in features:\n",
    "            if isinstance(sublist, str):\n",
    "                all_features.add(sublist)\n",
    "            else:\n",
    "                for item in sublist:\n",
    "                    all_features.add(item)\n",
    "        return all_features\n",
    "\n",
    "    if features is None:\n",
    "        # each feature in its own group\n",
    "        features = X_valid.columns.values\n",
    "    else:\n",
    "        req_feature_set = flatten(features)\n",
    "        model_feature_set = set(X_valid.columns.values)\n",
    "        # any features left over?\n",
    "        other_feature_set = model_feature_set.difference(req_feature_set)\n",
    "        if len(other_feature_set) > 0:\n",
    "            # if leftovers, we need group together as single new feature\n",
    "            features.append(list(other_feature_set))\n",
    "\n",
    "    X_valid, y_valid, sample_weights = sample(X_valid, y_valid, n_samples, sample_weights=sample_weights)\n",
    "    X_valid = X_valid.copy(deep=False)  # we're modifying columns\n",
    "\n",
    "    if callable(metric):\n",
    "        baseline = metric(model, X_valid, y_valid, sample_weights)\n",
    "    else:\n",
    "        baseline = model.score(X_valid, y_valid, sample_weights)\n",
    "\n",
    "    imp = []\n",
    "    for group in features:\n",
    "        if isinstance(group, str):\n",
    "            save = X_valid[group].copy()\n",
    "            X_valid[group] = np.random.permutation(X_valid[group])\n",
    "            if callable(metric):\n",
    "                m = metric(model, X_valid, y_valid, sample_weights)\n",
    "            else:\n",
    "                m = model.score(X_valid, y_valid, sample_weights)\n",
    "            X_valid[group] = save\n",
    "        else:\n",
    "            save = {}\n",
    "            for col in group:\n",
    "                save[col] = X_valid[col].copy()\n",
    "            for col in group:\n",
    "                X_valid[col] = np.random.permutation(X_valid[col])\n",
    "\n",
    "            if callable(metric):\n",
    "                m = metric(model, X_valid, y_valid, sample_weights)\n",
    "            else:\n",
    "                m = model.score(X_valid, y_valid, sample_weights)\n",
    "            for col in group:\n",
    "                X_valid[col] = save[col]\n",
    "        imp.append(baseline - m)\n",
    "\n",
    "    # Convert and groups/lists into string column names\n",
    "    labels = []\n",
    "    for col in features:\n",
    "        if isinstance(col, list):\n",
    "            labels.append('\\n'.join(col))\n",
    "        else:\n",
    "            labels.append(col)\n",
    "\n",
    "    I = pd.DataFrame(data={'Feature': labels, 'Importance': np.array(imp)})\n",
    "    I = I.set_index('Feature')\n",
    "    if sort:\n",
    "        I = I.sort_values('Importance', ascending=False)\n",
    "    return I\n",
    "\n",
    "\n",
    "def sample(X_valid, y_valid, n_samples, sample_weights=None):\n",
    "    if n_samples < 0: n_samples = len(X_valid)\n",
    "    n_samples = min(n_samples, len(X_valid))\n",
    "    if n_samples < len(X_valid):\n",
    "        ix = np.random.choice(len(X_valid), n_samples)\n",
    "        X_valid = X_valid.iloc[ix].copy(deep=False)  # shallow copy\n",
    "        y_valid = y_valid.iloc[ix].copy(deep=False)\n",
    "        if sample_weights is not None: sample_weights = sample_weights.iloc[ix].copy(deep=False)\n",
    "    return X_valid, y_valid, sample_weights\n",
    "\n",
    "\n",
    "def sample_rows(X, n_samples):\n",
    "    if n_samples < 0: n_samples = len(X)\n",
    "    n_samples = min(n_samples, len(X))\n",
    "    if n_samples < len(X):\n",
    "        ix = np.random.choice(len(X), n_samples)\n",
    "        X = X.iloc[ix].copy(deep=False)  # shallow copy\n",
    "    return X\n",
    "\n",
    "\n",
    "def oob_importances(rf, X_train, y_train, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Compute permutation feature importances for scikit-learn\n",
    "    RandomForestClassifier or RandomForestRegressor in arg rf.\n",
    "    Given training X and y data, return a data frame with columns\n",
    "    Feature and Importance sorted in reverse order by importance.\n",
    "    The training data is needed to compute out of bag (OOB)\n",
    "    model performance measures (accuracy or R^2). The model\n",
    "    is not retrained.\n",
    "    By default, sample up to 5000 observations to compute feature importances.\n",
    "    return: A data frame with Feature, Importance columns\n",
    "    SAMPLE CODE\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    X_train, y_train = ..., ...\n",
    "    rf.fit(X_train, y_train)\n",
    "    imp = oob_importances(rf, X_train, y_train)\n",
    "    \"\"\"\n",
    "    if isinstance(rf, RandomForestClassifier):\n",
    "        return permutation_importances(rf, X_train, y_train, oob_classifier_accuracy, n_samples)\n",
    "    elif isinstance(rf, RandomForestRegressor):\n",
    "        return permutation_importances(rf, X_train, y_train, oob_regression_r2_score, n_samples)\n",
    "    return None\n",
    "\n",
    "\n",
    "def cv_importances(model, X_train, y_train, k=3):\n",
    "    \"\"\"\n",
    "    Compute permutation feature importances for scikit-learn models using\n",
    "    k-fold cross-validation (default k=3).\n",
    "    Given a Classifier or Regressor in model\n",
    "    and training X and y data, return a data frame with columns\n",
    "    Feature and Importance sorted in reverse order by importance.\n",
    "    Cross-validation observations are taken from X_train, y_train.\n",
    "    The model.score() method is called to measure accuracy drops.\n",
    "    return: A data frame with Feature, Importance columns\n",
    "    SAMPLE CODE\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    X_train, y_train = ..., ...\n",
    "    rf.fit(X_train, y_train)\n",
    "    imp = cv_importances(rf, X_train, y_train)\n",
    "    \"\"\"\n",
    "    def score(model):\n",
    "        cvscore = cross_val_score(\n",
    "            model,  # which model to use\n",
    "            X_train, y_train,  # what training data to split up\n",
    "            cv=k)  # number of folds/chunks\n",
    "        return np.mean(cvscore)\n",
    "\n",
    "    X_train = X_train.copy(deep=False)  # shallow copy\n",
    "    baseline = score(model)\n",
    "    imp = []\n",
    "    for col in X_train.columns:\n",
    "        save = X_train[col].copy()\n",
    "        X_train[col] = np.random.permutation(X_train[col])\n",
    "        m = score(model)\n",
    "        X_train[col] = save\n",
    "        imp.append(baseline - m)\n",
    "\n",
    "    I = pd.DataFrame(data={'Feature': X_train.columns, 'Importance': np.array(imp)})\n",
    "    I = I.set_index('Feature')\n",
    "    I = I.sort_values('Importance', ascending=False)\n",
    "    return I\n",
    "\n",
    "\n",
    "def permutation_importances(rf, X_train, y_train, metric, n_samples=5000):\n",
    "    imp = permutation_importances_raw(rf, X_train, y_train, metric, n_samples)\n",
    "    I = pd.DataFrame(data={'Feature':X_train.columns, 'Importance':imp})\n",
    "    I = I.set_index('Feature')\n",
    "    I = I.sort_values('Importance', ascending=False)\n",
    "    return I\n",
    "\n",
    "\n",
    "def dropcol_importances(model, X_train, y_train, X_valid=None, y_valid=None, metric=None, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Compute drop-column feature importances for scikit-learn.\n",
    "    Given a classifier or regression in model\n",
    "    and training X and y data, return a data frame with columns\n",
    "    Feature and Importance sorted in reverse order by importance.\n",
    "    A clone of model is trained once to get the baseline score and then\n",
    "    again, once per feature to compute the drop in either the model's .score() output\n",
    "    or a custom metric callable in the form of metric(model, X_valid, y_valid).\n",
    "    In case of a custom metric the X_valid and y_valid parameters should be set.\n",
    "    return: A data frame with Feature, Importance columns\n",
    "    SAMPLE CODE\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    X_train, y_train = ..., ...\n",
    "    rf.fit(X_train, y_train)\n",
    "    imp = dropcol_importances(rf, X_train, y_train)\n",
    "    \"\"\"\n",
    "    if X_valid is None: X_valid = X_train\n",
    "    if y_valid is None: y_valid = y_train\n",
    "    model_ = clone(model)\n",
    "    model_.random_state = 999\n",
    "    model_.fit(X_train, y_train)\n",
    "    if callable(metric):\n",
    "        baseline = metric(model_, X_valid, y_valid, sample_weights)\n",
    "    else:\n",
    "        baseline = model_.score(X_valid, y_valid, sample_weights)\n",
    "    imp = []\n",
    "    for col in X_train.columns:\n",
    "        model_ = clone(model)\n",
    "        model_.random_state = 999\n",
    "        model_.fit(X_train.drop(col,axis=1), y_train)\n",
    "        if callable(metric):\n",
    "            s = metric(model_, X_valid.drop(col,axis=1), y_valid, sample_weights)\n",
    "        else:\n",
    "            s = model_.score(X_valid.drop(col,axis=1), y_valid, sample_weights)\n",
    "        drop_in_score = baseline - s\n",
    "        imp.append(drop_in_score)\n",
    "    imp = np.array(imp)\n",
    "    I = pd.DataFrame(data={'Feature':X_train.columns, 'Importance':imp})\n",
    "    I = I.set_index('Feature')\n",
    "    I = I.sort_values('Importance', ascending=False)\n",
    "    return I\n",
    "\n",
    "\n",
    "def oob_dropcol_importances(rf, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute drop-column feature importances for scikit-learn.\n",
    "    Given a RandomForestClassifier or RandomForestRegressor in rf\n",
    "    and training X and y data, return a data frame with columns\n",
    "    Feature and Importance sorted in reverse order by importance.\n",
    "    A clone of rf is trained once to get the baseline score and then\n",
    "    again, once per feature to compute the drop in out of bag (OOB)\n",
    "    score.\n",
    "    return: A data frame with Feature, Importance columns\n",
    "    SAMPLE CODE\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    X_train, y_train = ..., ...\n",
    "    rf.fit(X_train, y_train)\n",
    "    imp = oob_dropcol_importances(rf, X_train, y_train)\n",
    "    \"\"\"\n",
    "    rf_ = clone(rf)\n",
    "    rf_.random_state = 999\n",
    "    rf_.oob_score = True\n",
    "    rf_.fit(X_train, y_train)\n",
    "    baseline = rf_.oob_score_\n",
    "    imp = []\n",
    "    for col in X_train.columns:\n",
    "        rf_ = clone(rf)\n",
    "        rf_.random_state = 999\n",
    "        rf_.oob_score = True\n",
    "        rf_.fit(X_train.drop(col, axis=1), y_train)\n",
    "        drop_in_score = baseline - rf_.oob_score_\n",
    "        imp.append(drop_in_score)\n",
    "    imp = np.array(imp)\n",
    "    I = pd.DataFrame(data={'Feature':X_train.columns, 'Importance':imp})\n",
    "    I = I.set_index('Feature')\n",
    "    I = I.sort_values('Importance', ascending=False)\n",
    "    return I\n",
    "\n",
    "\n",
    "def importances_raw(rf, X_train, y_train, n_samples=5000):\n",
    "    if isinstance(rf, RandomForestClassifier):\n",
    "        return permutation_importances_raw(rf, X_train, y_train, oob_classifier_accuracy, n_samples)\n",
    "    elif isinstance(rf, RandomForestRegressor):\n",
    "        return permutation_importances_raw(rf, X_train, y_train, oob_regression_r2_score, n_samples)\n",
    "    return None\n",
    "\n",
    "\n",
    "def permutation_importances_raw(rf, X_train, y_train, metric, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Return array of importances from pre-fit rf; metric is function\n",
    "    that measures accuracy or R^2 or similar. This function\n",
    "    works for regressors and classifiers.\n",
    "    \"\"\"\n",
    "    X_sample, y_sample, _ = sample(X_train, y_train, n_samples)\n",
    "\n",
    "    if not hasattr(rf, 'estimators_'):\n",
    "        rf.fit(X_sample, y_sample)\n",
    "\n",
    "    baseline = metric(rf, X_sample, y_sample)\n",
    "    X_train = X_sample.copy(deep=False) # shallow copy\n",
    "    y_train = y_sample\n",
    "    imp = []\n",
    "    for col in X_train.columns:\n",
    "        save = X_train[col].copy()\n",
    "        X_train[col] = np.random.permutation(X_train[col])\n",
    "        m = metric(rf, X_train, y_train)\n",
    "        X_train[col] = save\n",
    "        drop_in_metric = baseline - m\n",
    "        imp.append(drop_in_metric)\n",
    "    return np.array(imp)\n",
    "\n",
    "\n",
    "def _get_unsampled_indices(tree, n_samples):\n",
    "    \"\"\"\n",
    "    An interface to get unsampled indices regardless of sklearn version.\n",
    "    \"\"\"\n",
    "    if LooseVersion(sklearn.__version__) >= LooseVersion(\"0.24\"):\n",
    "        # Version 0.24 moved forest package name\n",
    "        from sklearn.ensemble._forest import _get_n_samples_bootstrap\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "        return _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "    elif LooseVersion(sklearn.__version__) >= LooseVersion(\"0.22\"):\n",
    "        # Version 0.22 or newer uses 3 arguments.\n",
    "        from sklearn.ensemble.forest import _get_n_samples_bootstrap\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "        return _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "    else:\n",
    "        # Version 0.21 or older uses only two arguments.\n",
    "        return _generate_unsampled_indices(tree.random_state, n_samples)\n",
    "\n",
    "\n",
    "def oob_classifier_accuracy(rf, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute out-of-bag (OOB) accuracy for a scikit-learn random forest\n",
    "    classifier. We learned the guts of scikit's RF from the BSD licensed\n",
    "    code:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L425\n",
    "    \"\"\"\n",
    "    X = X_train.values\n",
    "    y = y_train.values\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_classes = len(np.unique(y))\n",
    "    predictions = np.zeros((n_samples, n_classes))\n",
    "    for tree in rf.estimators_:\n",
    "        unsampled_indices = _get_unsampled_indices(tree, n_samples)\n",
    "        tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "        predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "    predicted_class_indexes = np.argmax(predictions, axis=1)\n",
    "    predicted_classes = [rf.classes_[i] for i in predicted_class_indexes]\n",
    "\n",
    "    oob_score = np.mean(y == predicted_classes)\n",
    "    return oob_score\n",
    "\n",
    "\n",
    "def oob_classifier_f1_score(rf, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute out-of-bag (OOB) f1 score for a scikit-learn random forest\n",
    "    classifier. We learned the guts of scikit's RF from the BSD licensed\n",
    "    code:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L425\n",
    "    \"\"\"\n",
    "    X = X_train.values\n",
    "    y = y_train.values\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_classes = len(np.unique(y))\n",
    "    predictions = np.zeros((n_samples, n_classes))\n",
    "    for tree in rf.estimators_:\n",
    "        unsampled_indices = _get_unsampled_indices(tree, n_samples)\n",
    "        tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "        predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "    predicted_class_indexes = np.argmax(predictions, axis=1)\n",
    "    predicted_classes = [rf.classes_[i] for i in predicted_class_indexes]\n",
    "\n",
    "    oob_score = f1_score(y, predicted_classes, average='macro')\n",
    "    return oob_score\n",
    "\n",
    "\n",
    "def oob_regression_r2_score(rf, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute out-of-bag (OOB) R^2 for a scikit-learn random forest\n",
    "    regressor. We learned the guts of scikit's RF from the BSD licensed\n",
    "    code:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L702\n",
    "    \"\"\"\n",
    "    X = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    y = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "\n",
    "    n_samples = len(X)\n",
    "    predictions = np.zeros(n_samples)\n",
    "    n_predictions = np.zeros(n_samples)\n",
    "    for tree in rf.estimators_:\n",
    "        unsampled_indices = _get_unsampled_indices(tree, n_samples)\n",
    "        tree_preds = tree.predict(X[unsampled_indices, :])\n",
    "        predictions[unsampled_indices] += tree_preds\n",
    "        n_predictions[unsampled_indices] += 1\n",
    "\n",
    "    if (n_predictions == 0).any():\n",
    "        warnings.warn(\"Too few trees; some variables do not have OOB scores.\")\n",
    "        n_predictions[n_predictions == 0] = 1\n",
    "\n",
    "    predictions /= n_predictions\n",
    "\n",
    "    oob_score = r2_score(y, predictions)\n",
    "    return oob_score\n",
    "\n",
    "\n",
    "def stemplot_importances(df_importances,\n",
    "                         yrot=0,\n",
    "                         label_fontsize=10,\n",
    "                         width=4,\n",
    "                         minheight=1.5,\n",
    "                         vscale=1.0,\n",
    "                         imp_range=(-.002, .15),\n",
    "                         color='#375FA5',\n",
    "                         bgcolor=None,  # seaborn uses '#F1F8FE'\n",
    "                         xtick_precision=2,\n",
    "                         title=None):\n",
    "    GREY = '#444443'\n",
    "    I = df_importances\n",
    "    unit = 1\n",
    "\n",
    "    imp = I.Importance.values\n",
    "    mindrop = np.min(imp)\n",
    "    maxdrop = np.max(imp)\n",
    "    imp_padding = 0.002\n",
    "    imp_range = (min(imp_range[0], mindrop - imp_padding), max(imp_range[1], maxdrop))\n",
    "\n",
    "    barcounts = np.array([f.count('\\n')+1 for f in I.index])\n",
    "    N = np.sum(barcounts)\n",
    "    ymax = N * unit\n",
    "    # print(f\"barcounts {barcounts}, N={N}, ymax={ymax}\")\n",
    "    height = max(minheight, ymax * .27 * vscale)\n",
    "\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(width,height))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(*imp_range)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.spines['top'].set_linewidth(.3)\n",
    "    ax.spines['right'].set_linewidth(.3)\n",
    "    ax.spines['left'].set_linewidth(.3)\n",
    "    ax.spines['bottom'].set_linewidth(.3)\n",
    "    if bgcolor:\n",
    "        ax.set_facecolor(bgcolor)\n",
    "\n",
    "    yloc = []\n",
    "    y = barcounts[0]*unit / 2\n",
    "    yloc.append(y)\n",
    "    for i in range(1,len(barcounts)):\n",
    "        wprev = barcounts[i-1]\n",
    "        w = barcounts[i]\n",
    "        y += (wprev + w)/2 * unit\n",
    "        yloc.append(y)\n",
    "    yloc = np.array(yloc)\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter(f'%.{xtick_precision}f'))\n",
    "    ax.set_xticks([maxdrop, imp_range[1]])\n",
    "    ax.tick_params(labelsize=label_fontsize, labelcolor=GREY)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=label_fontsize+1, fontname=\"Arial\", color=GREY)\n",
    "\n",
    "    plt.hlines(y=yloc, xmin=imp_range[0], xmax=imp, lw=barcounts*1.2, color=color)\n",
    "    for i in range(len(I.index)):\n",
    "        plt.plot(imp[i], yloc[i], \"o\", color=color, markersize=barcounts[i]+2)\n",
    "    ax.set_yticks(yloc)\n",
    "    ax.set_yticklabels(I.index, fontdict={'verticalalignment': 'center'})\n",
    "    plt.tick_params(\n",
    "        pad=0,\n",
    "        axis='y',\n",
    "        which='both',\n",
    "        left=False)\n",
    "\n",
    "    # rotate y-ticks\n",
    "    if yrot is not None:\n",
    "        plt.yticks(rotation=yrot)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return PimpViz()\n",
    "\n",
    "\n",
    "def plot_importances(df_importances,\n",
    "                     yrot=0,\n",
    "                     label_fontsize=10,\n",
    "                     width=4,\n",
    "                     minheight=1.5,\n",
    "                     vscale=1,\n",
    "                     imp_range=(-.002, .15),\n",
    "                     color='#D9E6F5',\n",
    "                     bgcolor=None,  # seaborn uses '#F1F8FE'\n",
    "                     xtick_precision=2,\n",
    "                     title=None,\n",
    "                     ax=None):\n",
    "    \"\"\"\n",
    "    Given an array or data frame of importances, plot a horizontal bar chart\n",
    "    showing the importance values.\n",
    "    :param df_importances: A data frame with Feature, Importance columns\n",
    "    :type df_importances: pd.DataFrame\n",
    "    :param width: Figure width in default units (inches I think). Height determined\n",
    "                  by number of features.\n",
    "    :type width: int\n",
    "    :param minheight: Minimum plot height in default matplotlib units (inches?)\n",
    "    :type minheight: float\n",
    "    :param vscale: Scale vertical plot (default .25) to make it taller\n",
    "    :type vscale: float\n",
    "    :param label_fontsize: Font size for feature names and importance values\n",
    "    :type label_fontsize: int\n",
    "    :param yrot: Degrees to rotate feature (Y axis) labels\n",
    "    :type yrot: int\n",
    "    :param label_fontsize:  The font size for the column names and x ticks\n",
    "    :type label_fontsize:  int\n",
    "    :param scalefig: Scale width and height of image (widthscale,heightscale)\n",
    "    :type scalefig: 2-tuple of floats\n",
    "    :param xtick_precision: How many digits after decimal for importance values.\n",
    "    :type xtick_precision: int\n",
    "    :param xtick_precision: Title of plot; set to None to avoid.\n",
    "    :type xtick_precision: string\n",
    "    :param ax: Matplotlib \"axis\" to plot into\n",
    "    :return: None\n",
    "    SAMPLE CODE\n",
    "    rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "    X_train, y_train = ..., ...\n",
    "    rf.fit(X_train, y_train)\n",
    "    imp = importances(rf, X_test, y_test)\n",
    "    viz = plot_importances(imp)\n",
    "    viz.save('file.svg')\n",
    "    viz.save('file.pdf')\n",
    "    viz.view() # or just viz in notebook\n",
    "    \"\"\"\n",
    "    I = df_importances\n",
    "    unit = 1\n",
    "    ypadding = .1\n",
    "\n",
    "    imp = I.Importance.values\n",
    "    mindrop = np.min(imp)\n",
    "    maxdrop = np.max(imp)\n",
    "    imp_padding = 0.002\n",
    "    imp_range = (min(imp_range[0], mindrop - imp_padding), max(imp_range[1], maxdrop + imp_padding))\n",
    "\n",
    "    barcounts = np.array([f.count('\\n')+1 for f in I.index])\n",
    "    N = np.sum(barcounts)\n",
    "    ymax = N * unit + len(I.index) * ypadding + ypadding\n",
    "    # print(f\"barcounts {barcounts}, N={N}, ymax={ymax}\")\n",
    "    height = max(minheight, ymax * .2 * vscale)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.close()\n",
    "        fig, ax = plt.subplots(1,1,figsize=(width,height))\n",
    "    ax.set_xlim(*imp_range)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.spines['top'].set_linewidth(.3)\n",
    "    ax.spines['right'].set_linewidth(.3)\n",
    "    ax.spines['left'].set_linewidth(.3)\n",
    "    ax.spines['bottom'].set_linewidth(.3)\n",
    "    if bgcolor:\n",
    "        ax.set_facecolor(bgcolor)\n",
    "\n",
    "    yloc = []\n",
    "    y = barcounts[0]*unit / 2 + ypadding\n",
    "    yloc.append(y)\n",
    "    for i in range(1,len(barcounts)):\n",
    "        wprev = barcounts[i-1]\n",
    "        w = barcounts[i]\n",
    "        y += (wprev + w)/2 * unit + ypadding\n",
    "        yloc.append(y)\n",
    "    yloc = np.array(yloc)\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter(f'%.{xtick_precision}f'))\n",
    "    # too close to show both max and right edge?\n",
    "    if maxdrop/imp_range[1] > 0.9 or maxdrop < 0.02:\n",
    "        ax.set_xticks([0, imp_range[1]])\n",
    "    else:\n",
    "        ax.set_xticks([0, maxdrop, imp_range[1]])\n",
    "    ax.tick_params(labelsize=label_fontsize, labelcolor=GREY)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=label_fontsize+1, fontname=\"Arial\", color=GREY)\n",
    "\n",
    "    barcontainer = ax.barh(y=yloc, width=imp,\n",
    "                           height=barcounts*unit,\n",
    "                           tick_label=I.index,\n",
    "                           color=color, align='center')\n",
    "\n",
    "    # Alter appearance of each bar\n",
    "    for rect in barcontainer.patches:\n",
    "            rect.set_linewidth(.5)\n",
    "            rect.set_edgecolor(GREY)\n",
    "\n",
    "    # rotate y-ticks\n",
    "    if yrot is not None:\n",
    "        ax.tick_params(labelrotation=yrot)\n",
    "\n",
    "    return PimpViz()\n",
    "\n",
    "\n",
    "def oob_dependences(rf, X_train, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Given a random forest model, rf, and training observation independent\n",
    "    variables in X_train (a dataframe), compute the OOB R^2 score using each var\n",
    "    as a dependent variable. We retrain rf for each var.    Only numeric columns are considered.\n",
    "    By default, sample up to 5000 observations to compute feature dependencies.\n",
    "    :return: Return a DataFrame with Feature/Dependence values for each variable. Feature is the dataframe index.\n",
    "    \"\"\"\n",
    "    numcols = [col for col in X_train if is_numeric_dtype(X_train[col])]\n",
    "\n",
    "    X_train = sample_rows(X_train, n_samples)\n",
    "\n",
    "    df_dep = pd.DataFrame(columns=['Feature','Dependence'])\n",
    "    df_dep = df_dep.set_index('Feature')\n",
    "    for col in numcols:\n",
    "        X, y = X_train.drop(col, axis=1), X_train[col]\n",
    "        rf.fit(X, y)\n",
    "        df_dep.loc[col] = rf.oob_score_\n",
    "    df_dep = df_dep.sort_values('Dependence', ascending=False)\n",
    "    return df_dep\n",
    "\n",
    "\n",
    "def feature_dependence_matrix(X_train,\n",
    "                              rfrmodel=RandomForestRegressor(n_estimators=50, oob_score=True),\n",
    "                              rfcmodel=RandomForestClassifier(n_estimators=50, oob_score=True),\n",
    "                              cat_count=20,\n",
    "                              zero=0.001,\n",
    "                              sort_by_dependence=False,\n",
    "                              n_samples=5000):\n",
    "    \"\"\"\n",
    "    Given training observation independent variables in X_train (a dataframe),\n",
    "    compute the feature importance using each var as a dependent variable using\n",
    "    a RandomForestRegressor or RandomForestClassifier. A RandomForestClassifer is \n",
    "    used when the number of the unique values for the dependent variable is less or \n",
    "    equal to the cat_count arg. We retrain a random forest for each var as target \n",
    "    using the others as independent vars. Only numeric columns are considered.\n",
    "    By default, sample up to 5000 observations to compute feature dependencies.\n",
    "    If feature importance is less than zero arg, force to 0. Force all negatives to 0.0.\n",
    "    Clip to 1.0 max. (Some importances could come back > 1.0 because removing that\n",
    "    feature sends R^2 very negative.)\n",
    "    :return: a non-symmetric data frame with the dependence matrix where each row is the importance of each var to the row's var used as a model target.\n",
    "    \"\"\"\n",
    "    numeric_cols = [col for col in X_train if is_numeric_dtype(X_train[col])]\n",
    "\n",
    "    cat_cols = [col for col in numeric_cols if X_train[col].value_counts().count() <= cat_count]\n",
    "    cat_cols_le = [col for col in cat_cols if X_train[col].dtypes == 'float' ]\n",
    "    for col in cat_cols_le:\n",
    "        le = LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col])\n",
    "\n",
    "    X_train = sample_rows(X_train, n_samples)\n",
    "\n",
    "    df_dep = pd.DataFrame(index=X_train.columns, columns=['Dependence']+X_train.columns.tolist())\n",
    "    for i,col in enumerate(numeric_cols):\n",
    "        X, y = X_train.drop(col, axis=1), X_train[col]\n",
    "        if col in cat_cols:\n",
    "            rf = clone(rfcmodel)\n",
    "            rf.fit(X,y)\n",
    "            imp = permutation_importances_raw(rf, X, y, oob_classifier_f1_score, n_samples)\n",
    "        else:\n",
    "            rf = clone(rfrmodel)\n",
    "            rf.fit(X,y)\n",
    "            imp = permutation_importances_raw(rf, X, y, oob_regression_r2_score, n_samples)\n",
    "        \"\"\"\n",
    "        Some RandomForestRegressor importances could come back > 1.0 because removing\n",
    "        that feature sends R^2 very negative. Clip them at 1.0.  Also, features with \n",
    "        negative importance means that taking them out helps predict but we don't care\n",
    "        about that here. We want to know which features are collinear/predictive. Clip\n",
    "        at 0.0.\n",
    "        \"\"\"\n",
    "        imp = np.clip(imp, a_min=0.0, a_max=1.0)\n",
    "        imp[imp<zero] = 0.0\n",
    "        imp = np.insert(imp, i, 1.0)\n",
    "        df_dep.iloc[i] = np.insert(imp, 0, rf.oob_score_) # add overall dependence\n",
    "\n",
    "    if sort_by_dependence:\n",
    "        return df_dep.sort_values('Dependence', ascending=False)\n",
    "    return df_dep\n",
    "\n",
    "\n",
    "def plot_dependence_heatmap(D,\n",
    "                            color_threshold=0.6,\n",
    "                            threshold=0.03,\n",
    "                            cmap=None,\n",
    "                            figsize=None,\n",
    "                            value_fontsize=8,\n",
    "                            label_fontsize=9,\n",
    "                            precision=2,\n",
    "                            xrot=70,\n",
    "                            grid=True):\n",
    "    depdata = D.values.astype(float)\n",
    "\n",
    "    ncols, nrows = depdata.shape\n",
    "    if figsize:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "    colnames = list(D.columns.values)\n",
    "    colnames[0] = \"$\\\\bf \"+colnames[0]+\"$\" # bold Dependence word\n",
    "    plt.xticks(range(len(colnames)), colnames, rotation=xrot, horizontalalignment='right',\n",
    "               fontsize=label_fontsize, color=GREY)\n",
    "    plt.yticks(range(len(colnames[1:])), colnames[1:], verticalalignment='center',\n",
    "               fontsize=label_fontsize, color=GREY)\n",
    "    if cmap is None:\n",
    "        cw = plt.get_cmap('coolwarm')\n",
    "        cmap = ListedColormap([cw(x) for x in np.arange(color_threshold, .85, 0.01)])\n",
    "    elif isinstance(cmap, str):\n",
    "        cmap = plt.get_cmap(cmap)\n",
    "    cm = copy(cmap)\n",
    "    cm.set_under(color='white')\n",
    "\n",
    "    for x in range(ncols):\n",
    "        for y in range(nrows):\n",
    "            if (x+1) == y or depdata[x,y]<threshold:\n",
    "                depdata[x,y] = 0\n",
    "\n",
    "    if grid:\n",
    "        plt.grid(True, which='major', alpha=.25)\n",
    "\n",
    "    im = plt.imshow(depdata, cmap=cm, vmin=color_threshold, vmax=1.0, aspect='equal')\n",
    "    cb = plt.colorbar(im,\n",
    "                      fraction=0.046,\n",
    "                      pad=0.04,\n",
    "                      ticks=[color_threshold,color_threshold+(1-color_threshold)/2,1.0])\n",
    "    cb.ax.tick_params(labelsize=label_fontsize, labelcolor=GREY, pad=0)\n",
    "    cb.outline.set_edgecolor('white')\n",
    "\n",
    "    plt.axvline(x=.5, lw=1, color=GREY)\n",
    "\n",
    "    for x in range(ncols):\n",
    "        for y in range(nrows):\n",
    "            if (x+1) == y:\n",
    "                plt.annotate('x', xy=(y, x),\n",
    "                             horizontalalignment='center',\n",
    "                             verticalalignment='center',\n",
    "                             fontsize=value_fontsize, color=GREY)\n",
    "            if (x+1) != y and not np.isclose(round(depdata[x, y],precision), 0.0):\n",
    "                plt.annotate(myround(depdata[x, y], precision), xy=(y, x),\n",
    "                             horizontalalignment='center',\n",
    "                             verticalalignment='center',\n",
    "                             fontsize=value_fontsize, color=GREY)\n",
    "    plt.tick_params(pad=0, axis='x', which='both')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_linewidth(.3)\n",
    "    ax.spines['right'].set_linewidth(.3)\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.spines['left'].set_edgecolor(GREY)\n",
    "    ax.spines['bottom'].set_linewidth(.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return PimpViz()\n",
    "\n",
    "\n",
    "def get_feature_corr(df, method=\"spearman\"):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        result = df.corr(method=method).values\n",
    "    elif callable(method):\n",
    "        result = method(df)\n",
    "    elif method == \"spearman\":\n",
    "        result = stats.spearmanr(df).correlation\n",
    "    elif method == \"pearson\":\n",
    "        result = np.corrcoef(df)\n",
    "    else:\n",
    "        raise ValueError(\"unsupported correlation method\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def feature_corr_matrix(df, method=\"spearman\"):\n",
    "    \"\"\"\n",
    "    Return the Spearman's rank-order correlation (or another method) between all pairs\n",
    "    of features as a matrix with feature names as index and column names.\n",
    "    The diagonal will be all 1.0 as features are self correlated.\n",
    "    Spearman's correlation is the same thing as converting two variables\n",
    "    to rank values and then running a standard Pearson's correlation\n",
    "    on those ranked variables. Spearman's is nonparametric and does not\n",
    "    assume a linear relationship between the variables; it looks for\n",
    "    monotonic relationships.\n",
    "    :param df: dataframe containing features as columns, and without the target variable.\n",
    "    :param method: A string (\"spearman\", \"pearson\") or a callable function.\n",
    "    :return: a data frame with the correlation matrix\n",
    "    \"\"\"\n",
    "    corr = np.round(get_feature_corr(df, method=method), 4)\n",
    "    df_corr = pd.DataFrame(data=corr, index=df.columns, columns=df.columns)\n",
    "    return df_corr\n",
    "\n",
    "\n",
    "def plot_corr_heatmap(df,\n",
    "                      color_threshold=0.6,\n",
    "                      cmap=None,\n",
    "                      figsize=None,\n",
    "                      value_fontsize=8,\n",
    "                      label_fontsize=9,\n",
    "                      precision=2,\n",
    "                      xrot=80,\n",
    "                      method=\"spearman\"):\n",
    "    \"\"\"\n",
    "    Display the feature spearman's correlation matrix as a heatmap with\n",
    "    any abs(value)>color_threshold appearing with background color.\n",
    "    Spearman's correlation is the same thing as converting two variables\n",
    "    to rank values and then running a standard Pearson's correlation\n",
    "    on those ranked variables. Spearman's is nonparametric and does not\n",
    "    assume a linear relationship between the variables; it looks for\n",
    "    monotonic relationships.\n",
    "    SAMPLE CODE\n",
    "    from rfpimp import plot_corr_heatmap\n",
    "    viz = plot_corr_heatmap(df_train, save='/tmp/corrheatmap.svg',\n",
    "                      figsize=(7,5), label_fontsize=13, value_fontsize=11)\n",
    "    viz.view() # or just viz in notebook\n",
    "    \"\"\"\n",
    "    corr = get_feature_corr(df, method=method)\n",
    "    if len(corr.shape) == 0:\n",
    "        corr = np.array([[1.0, corr],\n",
    "                         [corr, 1.0]])\n",
    "\n",
    "    filtered = copy(corr)\n",
    "    filtered = np.abs(filtered)  # work with abs but display negatives later\n",
    "    mask = np.ones_like(corr)\n",
    "    filtered[np.tril_indices_from(mask)] = -9999\n",
    "\n",
    "    if cmap is None:\n",
    "        cw = plt.get_cmap('coolwarm')\n",
    "        cmap = ListedColormap([cw(x) for x in np.arange(color_threshold, .85, 0.01)])\n",
    "    elif isinstance(cmap, str):\n",
    "        cmap = plt.get_cmap(cmap)\n",
    "    cm = copy(cmap)\n",
    "    cm.set_under(color='white')\n",
    "\n",
    "    if figsize:\n",
    "        plt.figure(figsize=figsize)\n",
    "    im = plt.imshow(filtered, cmap=cm, vmin=color_threshold, vmax=1, aspect='equal')\n",
    "\n",
    "    width, height = filtered.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if x == y:\n",
    "                plt.annotate('x', xy=(y, x),\n",
    "                             horizontalalignment='center',\n",
    "                             verticalalignment='center',\n",
    "                             fontsize=value_fontsize, color=GREY)\n",
    "            if x < y:\n",
    "                plt.annotate(myround(corr[x, y], precision), xy=(y, x),\n",
    "                             horizontalalignment='center',\n",
    "                             verticalalignment='center',\n",
    "                             fontsize=value_fontsize, color=GREY)\n",
    "\n",
    "    cb = plt.colorbar(im, fraction=0.046, pad=0.04, ticks=[color_threshold, color_threshold + (1 - color_threshold) / 2, 1.0])\n",
    "    cb.ax.tick_params(labelsize=label_fontsize, labelcolor=GREY, )\n",
    "    cb.outline.set_edgecolor('white')\n",
    "    plt.xticks(range(width), df.columns, rotation=xrot, horizontalalignment='right',\n",
    "               fontsize=label_fontsize, color=GREY)\n",
    "    plt.yticks(range(width), df.columns, verticalalignment='center',\n",
    "               fontsize=label_fontsize, color=GREY)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_linewidth(.3)\n",
    "    ax.spines['right'].set_linewidth(.3)\n",
    "    ax.spines['left'].set_linewidth(.3)\n",
    "    ax.spines['bottom'].set_linewidth(.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return PimpViz()\n",
    "\n",
    "\n",
    "def rfnnodes(rf):\n",
    "    \"\"\"Return the total number of decision and leaf nodes in all trees of the forest.\"\"\"\n",
    "    return sum(t.tree_.node_count for t in rf.estimators_)\n",
    "\n",
    "\n",
    "def dectree_max_depth(tree):\n",
    "    \"\"\"\n",
    "    Return the max depth of this tree in terms of how many nodes; a single\n",
    "    root node gives height 1.\n",
    "    \"\"\"\n",
    "    children_left = tree.children_left\n",
    "    children_right = tree.children_right\n",
    "\n",
    "    def walk(node_id):\n",
    "        if (children_left[node_id] != children_right[node_id]): # decision node\n",
    "            left_max = 1 + walk(children_left[node_id])\n",
    "            right_max = 1 + walk(children_right[node_id])\n",
    "            return max(left_max, right_max)\n",
    "        else:  # leaf\n",
    "            return 1\n",
    "\n",
    "    root_node_id = 0\n",
    "    return walk(root_node_id)\n",
    "\n",
    "\n",
    "def rfmaxdepths(rf):\n",
    "    \"\"\"\n",
    "    Return the max depth of all trees in rf forest in terms of how many nodes\n",
    "    (a single root node for a single tree gives height 1)\n",
    "    \"\"\"\n",
    "    return [dectree_max_depth(t.tree_) for t in rf.estimators_]\n",
    "\n",
    "\n",
    "def jeremy_trick_RF_sample_size(n):\n",
    "    if LooseVersion(sklearn.__version__) >= LooseVersion(\"0.24\"):\n",
    "        forest._generate_sample_indices = \\\n",
    "            (lambda rs, n_samples, _:\n",
    "             forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "    else:\n",
    "        forest._generate_sample_indices = \\\n",
    "            (lambda rs, n_samples: forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "\n",
    "def jeremy_trick_reset_RF_sample_size():\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n",
    "\n",
    "def myround(v,ndigits=2):\n",
    "    if np.isclose(v, 0.0):\n",
    "        return \"0\"\n",
    "    return format(v, '.' + str(ndigits) + 'f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
